{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoZm_HBndS9X"
      },
      "source": [
        "#1. Повнозв'язані нейронні мережі\n",
        "Візьміть дані, з якими ви працювали в лабораторній №1.  Побудуйте повнозв’язану нейронну мережу\n",
        "прямого поширення для задачі класифікації.\n",
        "Навчіть її на тренувальній вибірці, протестуйте на тестовій. Порівняйте результати з алгоритмами з Lab 1.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFjbxSVfdS9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53ec623-fb08-4e7a-d717-5c92d85806b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/500, Loss: 1.4150\n",
            "Epoch 100/500, Loss: 0.8282\n",
            "Epoch 150/500, Loss: 0.6528\n",
            "Epoch 200/500, Loss: 0.5953\n",
            "Epoch 250/500, Loss: 0.4604\n",
            "Epoch 300/500, Loss: 0.4522\n",
            "Epoch 350/500, Loss: 0.3984\n",
            "Epoch 400/500, Loss: 0.3352\n",
            "Epoch 450/500, Loss: 0.3010\n",
            "Epoch 500/500, Loss: 0.2770\n",
            "\n",
            "Accuracy on test set: 0.7674\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.64      0.67        14\n",
            "           1       0.77      0.67      0.71        15\n",
            "           2       0.75      1.00      0.86         3\n",
            "           4       1.00      1.00      1.00         3\n",
            "           5       0.67      1.00      0.80         2\n",
            "           6       0.86      1.00      0.92         6\n",
            "\n",
            "    accuracy                           0.77        43\n",
            "   macro avg       0.79      0.88      0.83        43\n",
            "weighted avg       0.77      0.77      0.76        43\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Завантаження та підготовка даних\n",
        "url='https://drive.google.com/file/d/1QFzghQ5iBStsVt8ktLIAVFBK6RsppeHp/view?usp=sharing'\n",
        "url_='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "df = pd.read_csv(url_)\n",
        "\n",
        "df = df.drop('Id', axis=1)\n",
        "X = df.drop('Type of glass', axis=1).values\n",
        "y = df['Type of glass'].values - 1  # класи 0..6\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train, X_test, y_train, y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)\n",
        "\n",
        "# Ваги класів (ручне створення)\n",
        "# обчислимо ваги по кількості елементів у train\n",
        "train_classes, counts = np.unique(y_train.cpu().numpy(), return_counts=True)\n",
        "class_weights_np = np.ones(7, dtype=np.float32)  # всі класи = 1 по замовчуванню\n",
        "for cls, cnt in zip(train_classes, counts):\n",
        "    class_weights_np[cls] = len(y_train) / (len(train_classes) * cnt)\n",
        "\n",
        "class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
        "\n",
        "# Модель\n",
        "class GlassClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(9, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 7)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "model = GlassClassifier().to(device)\n",
        "\n",
        "# Loss та оптимізатор\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Тренування\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Оцінка\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = torch.argmax(model(X_test), dim=1)\n",
        "    acc = accuracy_score(y_test.cpu(), y_pred.cpu())\n",
        "    print(f\"\\nAccuracy on test set: {acc:.4f}\\n\")\n",
        "    print(\"Classification Report:\\n\")\n",
        "    print(classification_report(y_test.cpu(), y_pred.cpu(), zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Повнозв’язана нейромережа випередила SVM, Decision Tree та AdaBoost, але\n",
        "поступилася kNN (0.79) і RandomForest (0.81).\n",
        "---\n",
        "Мережа продемонструвала стабільне зменшення loss протягом навчання.\n",
        "---\n",
        "FNN показала конкурентний результат, але для табличних даних класичні алгоритми (RandomForest, kNN) часто працюють краще – що й проявилось у цьому експерименті.\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "VIT9oqLSA7kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Згорткові нейронні мережі\n",
        "а) Побудуйте просту згорткову нейронну мережу\n",
        "(2–3 convolutional шари + fully connected). Навчіть її на обраному вами датасеті.\n",
        "---\n",
        "б) Використайте попередньо натреновану архітектуру (наприклад, ResNet, VGG, MobileNet). Замініть вихідний класифікатор\n",
        "на новий під ваші класи. Проведіть донавчання моделі на вашому датасеті.\n",
        "---\n",
        "Порівняйте результати\n",
        "(точність, швидкість збіжності, кількость даних).\n",
        "---"
      ],
      "metadata": {
        "id": "rXfZXHiXrETz"
      }
    },
    {
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import gdown\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "# Завантаження архіву\n",
        "file_id = \"1r_wdQGJLAMXKAYMf3OeEQb_7k9VJ3fp3\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "output = \"Intel_Image_Classification.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "!unzip -q Intel_Image_Classification.zip -d ./data/\n",
        "\n",
        "# Перевіримо структуру\n",
        "print(\"Folders in ./data/:\", os.listdir(\"./data\"))\n",
        "\n",
        "train_src = \"./data/seg_train/seg_train/\"\n",
        "test_src  = \"./data/seg_test/seg_test/\"\n",
        "pred_src  = \"./data/seg_pred/seg_pred/\"\n",
        "\n",
        "\n",
        "print(\"Train classes:\", os.listdir(train_src))\n",
        "print(\"Test classes:\", os.listdir(test_src))\n",
        "print(\"Pred sample files:\", os.listdir(pred_src)[:10])\n",
        "\n",
        "\n",
        "# DATASET + DATALOADER\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((150,150)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_src, transform=transform)\n",
        "test_dataset  = datasets.ImageFolder(test_src, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Num train images:\", len(train_dataset))\n",
        "print(\"Num test images:\", len(test_dataset))\n",
        "\n",
        "\n",
        "# Проста CNN модель\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*18*18, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = SimpleCNN(num_classes=6).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "# Навчання CNN\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Тест\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"[CNN {epoch+1}/{epochs}] Loss={total_loss:.3f}, Test Acc={acc*100:.2f}%\")\n",
        "\n",
        "\n",
        "# RESNET18 TRANSFER LEARNING\n",
        "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, 6)\n",
        "resnet = resnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=1e-4)\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    resnet.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = resnet(x)\n",
        "        loss = criterion(preds, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # on test\n",
        "    resnet.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = resnet(x).argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"[ResNet {epoch+1}/{epochs}] Loss={total_loss:.3f}, Test Acc={acc*100:.2f}%\")\n"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1r_wdQGJLAMXKAYMf3OeEQb_7k9VJ3fp3\n",
            "From (redirected): https://drive.google.com/uc?id=1r_wdQGJLAMXKAYMf3OeEQb_7k9VJ3fp3&confirm=t&uuid=b90e1062-e7dd-4233-96dd-0fcf8c619241\n",
            "To: /content/Intel_Image_Classification.zip\n",
            "100%|██████████| 363M/363M [00:06<00:00, 53.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders in ./data/: ['seg_train', 'seg_pred', 'seg_test']\n",
            "Train classes: ['sea', 'street', 'forest', 'buildings', 'mountain', 'glacier']\n",
            "Test classes: ['sea', 'street', 'forest', 'buildings', 'mountain', 'glacier']\n",
            "Pred sample files: ['13261.jpg', '13014.jpg', '12920.jpg', '8035.jpg', '22630.jpg', '3580.jpg', '5702.jpg', '11391.jpg', '7269.jpg', '20526.jpg']\n",
            "Num train images: 14034\n",
            "Num test images: 3000\n",
            "[CNN 1/5] Loss=353.371, Test Acc=78.27%\n",
            "[CNN 2/5] Loss=217.706, Test Acc=82.40%\n",
            "[CNN 3/5] Loss=154.922, Test Acc=83.33%\n",
            "[CNN 4/5] Loss=102.363, Test Acc=80.83%\n",
            "[CNN 5/5] Loss=67.006, Test Acc=82.17%\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 148MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResNet 1/3] Loss=134.471, Test Acc=92.27%\n",
            "[ResNet 2/3] Loss=54.523, Test Acc=93.00%\n",
            "[ResNet 3/3] Loss=27.124, Test Acc=93.27%\n"
          ]
        }
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-l7e7Tr-BGY",
        "outputId": "61795177-b92c-4032-cfc3-f2823441e421"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Переднавчена ResNet18 перевершила власну CNN на ≈10%, а також тренувалась набагато швидше та стабільніше.\n",
        "---\n",
        "Transfer learning показує свою ефективність, особливо на реальних великих зображеннях.\n",
        "---\n",
        "Використання великої попередньо натренованої архітектури дає кращий початковий простір ознак, тому навчання потребує менше даних і часу.\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "FoiJlFkLBMbY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPBqC8-8dS9c"
      },
      "source": [
        "# 3. Вирішіть задачу класифікації текстів (використайте той же датасет, з яким ви працювали в лабораторній № 2) двома способами:\n",
        "\n",
        "а) Побудуйте модель з вбудованим Embedding шаром (ініціалізованим випадковими вагами). Використайте RNN / LSTM / GRU для класифікації. Навчіть модель на вашому датасеті.\n",
        "---\n",
        "б) Завантажте готові embeddings (наприклад, GloVe). Ініціалізуйте Embedding шар цими вагами.\n",
        "Проведіть навчання.\n",
        "---\n",
        "Порівняйте якість класифікації у (а) та (б). Чи покращилися метрики\n",
        "при використанні pretrained embeddings? Наскільки швидше/стабільніше відбулося\n",
        "навчання?\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01U7ytu6dS9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9943992a-c105-455f-ab80-1d764efef884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1IwcM9-Dyir1O2JhOxgnRo3Tb9xAt8lWP\n",
            "From (redirected): https://drive.google.com/uc?id=1IwcM9-Dyir1O2JhOxgnRo3Tb9xAt8lWP&confirm=t&uuid=8f00f3b3-0559-461d-9a1f-4b9642a8be53\n",
            "To: /content/reviews.txt\n",
            "100% 177M/177M [00:05<00:00, 35.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3336925858.py:37: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  data['label'] = data['label'].replace({'__label__1': 0, '__label__2': 1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Basic] Epoch 1: train_loss=0.2909, val_loss=0.2200, val_acc=91.30%\n",
            "[Basic] Epoch 2: train_loss=0.1905, val_loss=0.1960, val_acc=92.46%\n",
            "[Basic] Epoch 3: train_loss=0.1597, val_loss=0.1908, val_acc=92.57%\n",
            "[Basic] Epoch 4: train_loss=0.1369, val_loss=0.1919, val_acc=92.84%\n",
            "[Basic] Epoch 5: train_loss=0.1175, val_loss=0.1956, val_acc=92.81%\n",
            "GloVe coverage: 19746/20000 = 98.7%\n",
            "[GloVe] Epoch 1: train_loss=0.3274, val_loss=0.2951, val_acc=87.38%\n",
            "[GloVe] Epoch 2: train_loss=0.2377, val_loss=0.2217, val_acc=91.04%\n",
            "[GloVe] Epoch 3: train_loss=0.2087, val_loss=0.2108, val_acc=91.63%\n",
            "[GloVe] Epoch 4: train_loss=0.1922, val_loss=0.1957, val_acc=92.28%\n",
            "[GloVe] Epoch 5: train_loss=0.1797, val_loss=0.1949, val_acc=92.32%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import random\n",
        "\n",
        "# Підготовка середовища\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Завантажуємо дані\n",
        "!gdown --id 1IwcM9-Dyir1O2JhOxgnRo3Tb9xAt8lWP -O reviews.txt\n",
        "\n",
        "# Читаємо пострічково\n",
        "with open(\"reviews.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().splitlines()\n",
        "\n",
        "# Створюємо DataFrame\n",
        "import pandas as pd\n",
        "data = pd.DataFrame(lines, columns=['text'])\n",
        "\n",
        "# Витягуємо label і текст\n",
        "data['label'] = data['text'].apply(lambda x: x.split(' ')[0])\n",
        "data['text']  = data['text'].apply(lambda x: ' '.join(x.split(' ')[1:]))\n",
        "data['label'] = data['label'].replace({'__label__1': 0, '__label__2': 1})\n",
        "\n",
        "\n",
        "# Очищення тексту\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"@\\S+\", \" \", text)\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "data['clean_text'] = data['text'].apply(clean_text)\n",
        "\n",
        "# Токенізація (проста, без punkt_tab)\n",
        "tokens = [t.split() for t in data['clean_text']]\n",
        "\n",
        "# Створюємо словник\n",
        "counter = Counter()\n",
        "for tok_list in tokens:\n",
        "    counter.update(tok_list)\n",
        "\n",
        "MAX_VOCAB = 20000\n",
        "specials = [\"[PAD]\", \"[UNK]\"]\n",
        "most_common = counter.most_common(MAX_VOCAB - len(specials))\n",
        "itos = specials + [w for w, _ in most_common]   # index -> string\n",
        "stoi = {w:i for i,w in enumerate(itos)}         # string -> index\n",
        "\n",
        "PAD_IDX = stoi[\"[PAD]\"]\n",
        "UNK_IDX = stoi[\"[UNK]\"]\n",
        "\n",
        "# Кодування та паддінг\n",
        "MAX_LEN = 100\n",
        "\n",
        "def encode(tokens):\n",
        "    return [stoi.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "def pad_sequence(seq):\n",
        "    seq = seq[:MAX_LEN] + [PAD_IDX] * max(0, MAX_LEN - len(seq))\n",
        "    return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "encoded_texts = [encode(tok_list) for tok_list in tokens]\n",
        "X = torch.stack([pad_sequence(seq) for seq in encoded_texts])\n",
        "y = torch.tensor(data['label'].values, dtype=torch.float32)\n",
        "\n",
        "# DataLoader\n",
        "dataset = TensorDataset(X, y)\n",
        "VAL_FRAC = 0.2\n",
        "val_sz = int(len(dataset) * VAL_FRAC)\n",
        "train_sz = len(dataset) - val_sz\n",
        "train_ds, val_ds = random_split(dataset, [train_sz, val_sz],\n",
        "                                generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=128)\n",
        "test_loader = val_loader  # тут використаємо val як тест для прикладу\n",
        "\n",
        "# Модель BiLSTM\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, (h, c) = self.lstm(x)\n",
        "        h_cat = torch.cat([h[-2], h[-1]], dim=1)\n",
        "        logits = self.fc(h_cat)\n",
        "        return logits.squeeze(1)\n",
        "\n",
        "# Функції навчання та оцінки\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        total_acc  += (preds == yb.long()).sum().item()\n",
        "        n += xb.size(0)\n",
        "    return total_loss/n, total_acc/n\n",
        "\n",
        "# Навчання без pretrained embeddings\n",
        "vocab_size = len(stoi)\n",
        "emb_dim = 100\n",
        "hidden_dim = 64\n",
        "\n",
        "model_basic = BiLSTM(vocab_size, emb_dim, hidden_dim, PAD_IDX).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_basic.parameters(), lr=1e-3)\n",
        "\n",
        "n_epochs = 5\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train_one_epoch(model_basic, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = evaluate(model_basic, val_loader, criterion, device)\n",
        "    print(f\"[Basic] Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc*100:.2f}%\")\n",
        "\n",
        "# Навчання з GloVe\n",
        "# Завантажуємо GloVe\n",
        "!wget -q http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n",
        "\n",
        "def load_glove_txt(path):\n",
        "    vectors = {}\n",
        "    with open(path, \"r\", encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(\" \")\n",
        "            word = parts[0]\n",
        "            vec = np.asarray(parts[1:], dtype=np.float32)\n",
        "            vectors[word] = vec\n",
        "    dim = len(next(iter(vectors.values())))\n",
        "    return vectors, dim\n",
        "\n",
        "glove, emb_dim = load_glove_txt(\"glove.6B.100d.txt\")\n",
        "\n",
        "# Збираємо embedding matrix\n",
        "emb_matrix = np.random.normal(scale=0.1, size=(vocab_size, emb_dim)).astype(np.float32)\n",
        "emb_matrix[PAD_IDX] = 0.0\n",
        "hit = 0\n",
        "for w, idx in stoi.items():\n",
        "    v = glove.get(w)\n",
        "    if v is not None:\n",
        "        emb_matrix[idx] = v\n",
        "        hit += 1\n",
        "print(f\"GloVe coverage: {hit}/{vocab_size} = {hit/vocab_size:.1%}\")\n",
        "\n",
        "pretrained_emb = nn.Embedding.from_pretrained(torch.tensor(emb_matrix),\n",
        "                                              freeze=True,\n",
        "                                              padding_idx=PAD_IDX).to(device)\n",
        "\n",
        "model_glove = BiLSTM(vocab_size, emb_dim, hidden_dim, PAD_IDX).to(device)\n",
        "model_glove.embedding = pretrained_emb\n",
        "\n",
        "optimizer = torch.optim.Adam(model_glove.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train_one_epoch(model_glove, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = evaluate(model_glove, val_loader, criterion, device)\n",
        "    print(f\"[GloVe] Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "У цьому конкретному датасеті власні Embedding показали трохи кращу метрику (≈ +0.5%).\n",
        "---\n",
        "GloVe дав:\n",
        "---\n",
        " - кращу семантичну ініціалізацію\n",
        " - плавніше навчання після 2–3 епох\n",
        " - але стартував гірше, тому фінальний результат був трохи нижчий.\n",
        "\n",
        "Це типова ситуація, коли великий датасет + проста структура => модель сама навчає оптимальні embeddings.\n",
        "---\n",
        "Різниця мінімальна, обидві моделі працюють дуже добре (>92%).\n",
        "---"
      ],
      "metadata": {
        "id": "pgHVCp5bBUmc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}